## Understanding LangChain Runnables

At its core, a **Runnable** in LangChain is essentially **any piece of functionality that can be strung together with other pieces to form a larger operation.** Think of it like Lego bricks: each brick (Runnable) does something specific, and you can connect them (chain them) to build a much larger and more complex structure (your LLM application).

Every Runnable follows a simple rule: **it takes an input, processes it, and produces an output.** The beauty of this is that the output of one Runnable can seamlessly become the input for the next. This chaining capability is fundamental to building complex LLM applications with LangChain Expression Language (LCEL).

-----

## Why Use Runnables? The Benefits\!

Using Runnables and LCEL offers a ton of advantages for building robust and efficient LLM applications:

  * **Streaming:** Runnables support **streaming data**, meaning you can get results piece by piece instead of waiting for the entire output. This is super useful for getting word-by-word responses from LLMs or seeing intermediate results in long chains.
  * **Asynchronous Support:** You can use `async/await` with Runnables for **asynchronous operations**, allowing your application to perform multiple tasks concurrently and remain responsive.
  * **Parallel Execution:** Need to run several operations at the same time? Runnables make **parallel processing** easy, significantly boosting performance.
  * **Fallback Capabilities:** If one model or tool fails, Runnables allow you to **automatically switch to a backup**, making your application much more resilient.
  * **Input/Output Schemas:** Define the specific **input and output types** for your Runnables. This helps catch errors early and makes your chains more reliable.
  * **Background Observation:** Monitor and debug every step of your chain, understanding its performance. This is made easy with integrations like LangSmith.

-----

## Types of Runnables and Examples

Almost everything in LangChain is a Runnable. Here are some key types:

### 1\. Chains

The most fundamental Runnables are **chains**. A chain can be anything from a simple LLM call (just sending a prompt to a model) to a sequence of more complex steps.

**Example:**

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.output_parsers import StrOutputParser
import os
from dotenv import load_dotenv

load_dotenv()

# 1. Runnable: Prompt Template
# Creates a list of messages
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are an excellent chef and recommend recipes based on user requests."),
    ("user", "Suggest a recipe for {topic}.")
])

# 2. Runnable: Model (LLM)
# Processes incoming messages and generates an output
llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0)

# 3. Runnable: Output Parser
# Converts the model's output (an AIMessage object) into a readable string
output_parser = StrOutputParser()

# Construct the chain: prompt -> llm -> output_parser
# The | operator (pipe operator) chains Runnables together,
# feeding the output of one into the input of the next.
chain = prompt | llm | output_parser

# Run the chain
response = chain.invoke({"topic": "tomato soup"})
print(response)
```

In this example:

  * `prompt` is a Runnable. Its input is a dictionary (`{"topic": "tomato soup"}`), and its output is the list of messages generated by `ChatPromptTemplate`.
  * `llm` is another Runnable. Its input is the list of messages, and its output is an `AIMessage` object.
  * `output_parser` is also a Runnable. Its input is the `AIMessage` object, and its output is a clean string.

The `|` operator (pipe operator) links these Runnables, forming a **unified chain**. This entire chain is also, in itself, a Runnable.

### 2\. Maps and Parallel Operations (`RunnableParallel`)

When you want to run multiple operations concurrently, you use **RunnableParallel**. This executes each sub-Runnable in parallel and returns their results as a dictionary.

**Example:**

```python
from langchain_core.runnables import RunnableParallel
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI
import os

api_key = os.getenv("GOOGLE_API_KEY")
llm = ChatGoogleGenerativeAI(model="gemini-pro", api_key=api_key)
parser = StrOutputParser()

# Create two different prompts
chef_prompt = ChatPromptTemplate.from_template("You are a chef. Suggest a recipe for '{topic}'.")
critic_prompt = ChatPromptTemplate.from_template("You are a food critic. Write a critique of a suggested recipe for '{topic}'.")

# Create two chains that will run in parallel
chef_chain = chef_prompt | llm | parser
critic_chain = critic_prompt | llm | parser

# Use RunnableParallel to run these two chains concurrently
combined_chain = RunnableParallel(
    recipe_suggestion=chef_chain,
    critique=critic_chain
)

# Run the combined chain with a single input
result = combined_chain.invoke({"topic": "pizza"})

print("--- Recipe Suggestion ---")
print(result["recipe_suggestion"])
print("\n--- Critique ---")
print(result["critique"])
```

In this example, two separate LLM calls are made in parallel under the keys `recipe_suggestion` and `critique`, and the results are returned as a dictionary.

### 3\. Functions (`RunnableLambda`)

If you want to use your own Python functions as Runnables, you can wrap them with **RunnableLambda**. This is a great way to add custom logic to your chains.

**Example:**

```python
from langchain_core.runnables import RunnableLambda
from langchain_core.prompts import ChatPromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.output_parsers import StrOutputParser
import os

api_key = os.getenv("GOOGLE_API_KEY")
llm = ChatGoogleGenerativeAI(model="gemini-pro", api_key=api_key)
parser = StrOutputParser()

# Our custom function
def to_uppercase(text: str) -> str:
    return text.upper()

# Prompt and LLM chain
initial_chain = ChatPromptTemplate.from_template("Tell me a short sentence about '{topic}'.") | llm | parser

# Convert our function into a Runnable
uppercase_runnable = RunnableLambda(to_uppercase)

# Chain: Send the output of the first chain to our uppercase function
final_chain = initial_chain | uppercase_runnable

response = final_chain.invoke({"topic": "sun"})
print(response) # You'd get an output like: "THE SUN IS A HOT STAR."
```

Here, the `to_uppercase` function is transformed into a Runnable using `RunnableLambda` and then chained with other Runnables.

-----

## In Summary

The "Runnable" concept in LangChain is a standardized way to take different pieces of your application (prompts, models, parsers, custom functions, etc.) and connect them together. The pipe (`|`) operator and constructs like `RunnableParallel` and `RunnableLambda` allow you to combine these components in a flexible and powerful manner.

Essentially, every Runnable takes an input and produces an output. If you can grasp this "input-output" loop, building complex flows with LangChain Expression Language (LCEL) will become much, much easier.

I hope this explanation has helped you understand LangChain Runnables better\! Do you have any other questions?